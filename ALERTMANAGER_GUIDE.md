# AlertManager Guide - Kubernetes Cluster Health Checker

Complete guide to understanding and configuring AlertManager for Kubernetes monitoring.

---

## ğŸ“‹ Table of Contents

1. [What is AlertManager?](#what-is-alertmanager)
2. [Why Use AlertManager?](#why-use-alertmanager)
3. [How AlertManager Works](#how-alertmanager-works)
4. [Configuration Examples](#configuration-examples)
5. [Accessing AlertManager](#accessing-alertmanager)
6. [Common Use Cases](#common-use-cases)
7. [Best Practices](#best-practices)

---

## ğŸ”” What is AlertManager?

AlertManager is the **notification engine** for Prometheus. While Prometheus detects problems, AlertManager handles:

- ğŸ“§ **Sending notifications** (Email, Slack, PagerDuty, etc.)
- ğŸ”„ **Deduplicating alerts** (no spam!)
- ğŸ“¦ **Grouping related alerts** (50 pods down = 1 notification)
- ğŸ”• **Silencing alerts** (during maintenance)
- ğŸš« **Inhibiting dependent alerts** (hide symptoms, show root cause)
- ğŸ¯ **Routing alerts** (critical â†’ PagerDuty, warning â†’ Slack)

---

## ğŸ¯ Why Use AlertManager?

### Problem 1: Prometheus Can't Send Notifications

**Without AlertManager:**
```
Prometheus: "CPU is at 95%!" 
You: "Great, but how do I know?" ğŸ¤·
```

**With AlertManager:**
```
Prometheus: "CPU is at 95%!"
AlertManager: 
  â†’ Sends email to ops-team@company.com
  â†’ Posts to #alerts Slack channel
  â†’ Pages on-call engineer via PagerDuty
You: "Got it! Investigating now." âœ…
```

---

### Problem 2: Alert Spam

**Without AlertManager:**
```
10:00 - Alert: Pod CrashLooping
10:01 - Alert: Pod CrashLooping (duplicate)
10:02 - Alert: Pod CrashLooping (duplicate)
...
10:59 - Alert: Pod CrashLooping (60th duplicate!)

Result: 60 notifications for 1 problem! ğŸ˜±
```

**With AlertManager:**
```
10:00 - Alert: Pod CrashLooping
10:05 - Update: Still CrashLooping
10:10 - Update: Still CrashLooping

Result: 1 alert + periodic updates âœ…
```

---

### Problem 3: Cascading Alerts

**Without AlertManager:**
```
Node goes down â†’
  âŒ NodeDown alert
  âŒ Pod1Down alert (because node is down)
  âŒ Pod2Down alert (because node is down)
  âŒ Pod3Down alert (because node is down)
  ... (50 pods)
  âŒ ServiceUnavailable alert (because pods are down)

Result: 52 alerts for 1 root cause! ğŸ˜±
```

**With AlertManager (Inhibition):**
```
Node goes down â†’
  âœ… NodeDown alert (root cause)
  â¸ï¸  Pod alerts inhibited (dependent)
  â¸ï¸  Service alerts inhibited (dependent)

Result: 1 alert for the root cause! âœ…
```

---

### Problem 4: Planned Maintenance

**Without AlertManager:**
```
You: "Upgrading nodes tonight, expect downtime"
11:00 PM - NodeDown alerts fire
11:00 PM - Team gets paged
Team: "Why are we getting woken up?!" ğŸ˜¡
```

**With AlertManager (Silencing):**
```
You: "Upgrading nodes tonight, expect downtime"
You: Silence NodeDown alerts for 2 hours
11:00 PM - NodeDown alerts fire
11:00 PM - AlertManager: "Silenced, not sending"
Team: Sleeps peacefully ğŸ˜´
```

---

## ğŸ—ï¸ How AlertManager Works

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        PROMETHEUS                            â”‚
â”‚                                                              â”‚
â”‚  1. Scrapes metrics from Kubernetes                          â”‚
â”‚  2. Evaluates alert rules every 15 seconds                   â”‚
â”‚  3. Fires alerts when conditions are met                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ HTTP POST /api/v1/alerts
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ALERTMANAGER                            â”‚
â”‚                                                              â”‚
â”‚  4. Receives alerts from Prometheus                          â”‚
â”‚  5. Deduplicates identical alerts                            â”‚
â”‚  6. Groups related alerts together                           â”‚
â”‚  7. Applies silencing rules                                  â”‚
â”‚  8. Applies inhibition rules                                 â”‚
â”‚  9. Routes to appropriate receivers                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚               â”‚               â”‚              â”‚
         â–¼               â–¼               â–¼              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Email  â”‚      â”‚ Slack  â”‚     â”‚PagerDuty â”‚   â”‚ Webhook â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Alert Lifecycle

```
1. INACTIVE â†’ Alert rule exists but condition not met
              Example: CPU < 90%

2. PENDING â†’ Condition met, waiting for duration
             Example: CPU > 90% for 30 seconds
             
3. FIRING â†’ Duration exceeded, alert sent to AlertManager
            Example: CPU > 90% for 2 minutes

4. RESOLVED â†’ Condition no longer met
              Example: CPU back to 60%
```

---

## âš™ï¸ Configuration Examples

### Example 1: Email Notifications

```yaml
# alertmanager.yaml
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@company.com'
  smtp_auth_username: 'alerts@company.com'
  smtp_auth_password: 'your-password'

route:
  receiver: 'email-team'
  group_by: ['alertname', 'namespace']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h

receivers:
  - name: 'email-team'
    email_configs:
      - to: 'ops-team@company.com'
        headers:
          Subject: 'ğŸš¨ Alert: {{ .GroupLabels.alertname }}'
```

**Result:**
- Alerts sent to ops-team@company.com
- Grouped by alert name and namespace
- Wait 30s before sending first alert
- Send updates every 5 minutes
- Repeat every 4 hours if still firing

---

### Example 2: Slack Notifications

```yaml
# alertmanager.yaml
global:
  slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

route:
  receiver: 'slack-alerts'
  group_by: ['severity']

receivers:
  - name: 'slack-alerts'
    slack_configs:
      - channel: '#alerts'
        title: 'ğŸš¨ {{ .GroupLabels.alertname }}'
        text: |
          *Alert:* {{ .GroupLabels.alertname }}
          *Severity:* {{ .GroupLabels.severity }}
          *Summary:* {{ .CommonAnnotations.summary }}
          *Description:* {{ .CommonAnnotations.description }}
        color: '{{ if eq .GroupLabels.severity "critical" }}danger{{ else }}warning{{ end }}'
```

**Result:**
- Alerts posted to #alerts Slack channel
- Color-coded: red for critical, yellow for warning
- Includes alert details and descriptions

---

### Example 3: Routing by Severity

```yaml
# alertmanager.yaml
route:
  receiver: 'default'
  group_by: ['alertname']
  
  routes:
    # Critical alerts â†’ PagerDuty
    - match:
        severity: critical
      receiver: 'pagerduty'
      group_wait: 10s
      repeat_interval: 1h
    
    # Warning alerts â†’ Slack
    - match:
        severity: warning
      receiver: 'slack'
      group_wait: 30s
      repeat_interval: 4h
    
    # Info alerts â†’ Email
    - match:
        severity: info
      receiver: 'email'
      group_wait: 5m
      repeat_interval: 24h

receivers:
  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: 'your-pagerduty-key'
  
  - name: 'slack'
    slack_configs:
      - channel: '#alerts'
        api_url: 'your-slack-webhook'
  
  - name: 'email'
    email_configs:
      - to: 'team@company.com'
```

**Result:**
- ğŸ”´ Critical â†’ PagerDuty (immediate page)
- ğŸŸ¡ Warning â†’ Slack (team notification)
- ğŸŸ¢ Info â†’ Email (daily digest)

---

### Example 4: Alert Inhibition

```yaml
# alertmanager.yaml
inhibit_rules:
  # If node is down, don't alert about pods on that node
  - source_match:
      alertname: 'NodeDown'
    target_match:
      alertname: 'PodDown'
    equal: ['node']
  
  # If namespace has issues, don't alert about individual pods
  - source_match:
      alertname: 'NamespaceDown'
    target_match_re:
      alertname: 'Pod.*'
    equal: ['namespace']
```

**Result:**
- NodeDown alert fires â†’ PodDown alerts on that node are suppressed
- NamespaceDown alert fires â†’ All pod alerts in that namespace are suppressed

---

### Example 5: Alert Grouping

```yaml
# alertmanager.yaml
route:
  receiver: 'default'
  
  # Group by namespace and severity
  group_by: ['namespace', 'severity']
  
  # Wait 30s to collect alerts before sending
  group_wait: 30s
  
  # Send updates every 5 minutes
  group_interval: 5m
  
  # Repeat notification every 4 hours
  repeat_interval: 4h
```

**Result:**
```
Instead of:
  âŒ Alert: pod-1 high CPU in production
  âŒ Alert: pod-2 high CPU in production
  âŒ Alert: pod-3 high CPU in production

You get:
  âœ… Alert: 3 pods have high CPU in production namespace
```

---

## ğŸŒ Accessing AlertManager

### Start Port Forward

```bash
# Option 1: Use our script
./scripts/start-port-forwards.sh

# Option 2: Manual port forward
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-alertmanager 9093:9093
```

### Access Web UI

```bash
# Open in browser
open http://localhost:9093

# Or visit manually
http://localhost:9093
```

### Web UI Features

1. **Alerts Tab** - View all active alerts
2. **Silences Tab** - Create/view/delete silences
3. **Status Tab** - View AlertManager configuration

---

## ğŸ’¡ Common Use Cases

### Use Case 1: Silence Alerts During Maintenance

**Scenario:** Upgrading Kubernetes nodes, expect downtime for 2 hours

**Steps:**
1. Open AlertManager UI: http://localhost:9093
2. Click **Silences** tab
3. Click **New Silence**
4. Fill in:
   - **Matchers:** `alertname=NodeDown`
   - **Duration:** 2h
   - **Creator:** your-name
   - **Comment:** "Planned node upgrade"
5. Click **Create**

**Result:** NodeDown alerts won't be sent for 2 hours

---

### Use Case 2: View Active Alerts

**Steps:**
1. Open AlertManager UI: http://localhost:9093
2. Click **Alerts** tab
3. View all firing alerts
4. Filter by:
   - Alert name
   - Severity
   - Namespace

---

### Use Case 3: Test Alert Notifications

```bash
# Send test alert to AlertManager
curl -X POST http://localhost:9093/api/v1/alerts -d '[
  {
    "labels": {
      "alertname": "TestAlert",
      "severity": "warning"
    },
    "annotations": {
      "summary": "This is a test alert"
    }
  }
]'
```

---

## ğŸ“Š Best Practices

### 1. Use Severity Levels

```yaml
# In Prometheus alert rules
- alert: HighCPU
  expr: cpu_usage > 90
  labels:
    severity: warning
  
- alert: CriticalCPU
  expr: cpu_usage > 95
  labels:
    severity: critical
```

### 2. Add Meaningful Annotations

```yaml
- alert: PodCrashLooping
  annotations:
    summary: "Pod {{ $labels.pod }} is crash looping"
    description: "Pod has restarted {{ $value }} times in the last 5 minutes"
    runbook_url: "https://wiki.company.com/runbooks/pod-crash"
```

### 3. Group Related Alerts

```yaml
route:
  group_by: ['namespace', 'alertname']
```

### 4. Set Appropriate Intervals

```yaml
route:
  group_wait: 30s        # Wait to collect similar alerts
  group_interval: 5m     # Send updates every 5 min
  repeat_interval: 4h    # Repeat every 4 hours
```

### 5. Use Inhibition for Root Causes

```yaml
inhibit_rules:
  - source_match:
      alertname: 'NodeDown'
    target_match_re:
      alertname: 'Pod.*|Service.*'
    equal: ['node']
```

---

## ğŸ” Troubleshooting

### Issue: Alerts not being sent

**Check:**
```bash
# View AlertManager logs
kubectl logs -n monitoring alertmanager-prometheus-kube-prometheus-alertmanager-0

# Check AlertManager config
kubectl get secret alertmanager-prometheus-kube-prometheus-alertmanager -n monitoring -o yaml
```

### Issue: Too many notifications

**Solution:** Increase `repeat_interval`
```yaml
route:
  repeat_interval: 12h  # Instead of 4h
```

### Issue: Missing important alerts

**Solution:** Check silences
```bash
# View active silences in UI
http://localhost:9093/#/silences
```

---

## ğŸ“š Additional Resources

- **Official Docs:** https://prometheus.io/docs/alerting/latest/alertmanager/
- **Configuration:** https://prometheus.io/docs/alerting/latest/configuration/
- **MONITORING_GUIDE.md** - Our monitoring setup
- **TROUBLESHOOTING.md** - Common issues

---

**AlertManager is essential for production-grade monitoring!** ğŸ””âœ…

